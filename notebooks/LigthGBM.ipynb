{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Recherche Predictive - Vision 2026 (LightGBM Ultra-Tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "data_prep",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions Train: (307, 35), Test: (30, 35)\n"
     ]
    }
   ],
   "source": [
    "# Chargement et Feature Engineering Avance\n",
    "df_adm = pd.read_csv('../data/raw/admissions_hopital_pitie_2024.csv')\n",
    "df_adm['date_entree'] = pd.to_datetime(df_adm['date_entree'])\n",
    "daily_data = df_adm.groupby('date_entree').size().rename('admissions').reset_index()\n",
    "daily_data = daily_data.set_index('date_entree').asfreq('D', fill_value=0)\n",
    "\n",
    "def create_features_advanced(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Cycliques de base\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df.index.month / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df.index.month / 12)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df.index.dayofweek / 7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df.index.dayofweek / 7)\n",
    "    \n",
    "    # Holidays (France 2024)\n",
    "    holidays = ['2024-01-01', '2024-04-01', '2024-05-01', '2024-05-08', \n",
    "                '2024-05-09', '2024-05-20', '2024-07-14', '2024-08-15', \n",
    "                '2024-11-01', '2024-11-11', '2024-12-25']\n",
    "    holiday_dates = pd.to_datetime(holidays)\n",
    "    df['is_holiday'] = df.index.strftime('%Y-%m-%d').isin(holidays).astype(int)\n",
    "    df['days_to_holiday'] = [(holiday_dates[holiday_dates >= d].min() - d).days if any(holiday_dates >= d) else 365 for d in df.index]\n",
    "    \n",
    "    # Calendrier etendu\n",
    "    df['dayofyear_sin'] = np.sin(2 * np.pi * df.index.dayofyear / 365)\n",
    "    df['dayofyear_cos'] = np.cos(2 * np.pi * df.index.dayofyear / 365)\n",
    "    df['weekofyear'] = df.index.isocalendar().week.astype(int)\n",
    "    df['dayofmonth'] = df.index.day\n",
    "    df['quarter'] = df.index.quarter\n",
    "    df['is_month_start'] = df.index.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df.index.is_month_end.astype(int)\n",
    "    \n",
    "    # Lags (Spectre large)\n",
    "    for l in [1, 2, 3, 4, 5, 6, 7, 14, 21, 28]:\n",
    "        df[f'lag{l}'] = df['admissions'].shift(l)\n",
    "    \n",
    "    # Rolling Stats (Context larges)\n",
    "    for w in [3, 7, 14, 28]:\n",
    "        df[f'roll_mean_{w}'] = df['admissions'].shift(1).rolling(window=w).mean()\n",
    "        df[f'roll_std_{w}'] = df['admissions'].shift(1).rolling(window=w).std()\n",
    "        df[f'roll_max_{w}'] = df['admissions'].shift(1).rolling(window=w).max()\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "features_df = create_features_advanced(daily_data)\n",
    "\n",
    "TARGET = 'admissions'\n",
    "FEATURES = [c for c in features_df.columns if c != TARGET]\n",
    "\n",
    "# Split pro (December for test)\n",
    "train_df = features_df.iloc[:-30]\n",
    "test_df = features_df.iloc[-30:]\n",
    "X_train, y_train = train_df[FEATURES], train_df[TARGET]\n",
    "X_test, y_test = test_df[FEATURES], test_df[TARGET]\n",
    "\n",
    "print(f\"Dimensions Train: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tuning",
   "metadata": {},
   "source": [
    "## 2. GridSearch Profond (Objectif MAE)\n",
    "L'utilisation de `objective='regression_l1'` est cruciale car elle force LightGBM a minimiser directement l'ecart absolu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution du tuning intensif...\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [2000, 3000],\n",
    "    'learning_rate': [0.005, 0.01],\n",
    "    'num_leaves': [63, 127],\n",
    "    'max_depth': [-1,20],\n",
    "    'min_child_samples': [5,20],\n",
    "    'feature_fraction': [0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Note: Cela peut prendre du temps \n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "grid = GridSearchCV(\n",
    "    lgb.LGBMRegressor(objective='regression_l1', random_state=42, verbose=-1), \n",
    "    param_grid, \n",
    "    cv=tscv, \n",
    "    scoring='neg_mean_absolute_error', \n",
    "    verbose=1, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Execution du tuning intensif...\")\n",
    "grid.fit(X_train, y_train)\n",
    "best_lgb = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation",
   "metadata": {},
   "source": [
    "## 3. Evaluation Finale (Objectif MAE ~ 10)\n",
    "Voyons si nous pouvons briser le plafond des 90 MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_lgb.predict(X_test)\n",
    "mae_val = -grid.best_score_\n",
    "mae_test = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Points de Tuning Trouves : {grid.best_params_}\")\n",
    "print(f\"MAE de Validation (Fold Moyen) : {mae_val:.2f}\")\n",
    "print(f\"MAE FINALE SUR TEST (Decembre) : {mae_test:.2f}\")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=test_df.index, y=y_test, name='Donnees Reelles', line=dict(color='#005ba1', width=3)))\n",
    "fig.add_trace(go.Scatter(x=test_df.index, y=y_pred, name='LightGBM Ultra-Tuned', line=dict(color='#ff7f0e', dash='dot', width=3)))\n",
    "fig.update_layout(title='Performance LightGBM : Objectif MAE Minimal', template='plotly_dark')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4896e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importance des variables pour comprendre les leviers de performance\n",
    "importances = pd.DataFrame({'feature': FEATURES, 'importance': best_lgb.feature_importances_})\n",
    "importances = importances.sort_values('importance', ascending=False).head(15)\n",
    "fig_imp = px.bar(importances, x='importance', y='feature', orientation='h', title='Top 15 Features Impactantes')\n",
    "fig_imp.update_layout(template='plotly_dark')\n",
    "fig_imp.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
